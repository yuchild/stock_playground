{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be9b57b",
   "metadata": {},
   "source": [
    "## Build Functions for ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da1a875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Requirments (Updated on 9/17/2024)\n",
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648f2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yfinance import Ticker\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "\n",
    "from src import elt as et\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7df301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(symbol, interval, period):\n",
    "    stock = Ticker(symbol)\n",
    "    stock_df = stock.history(interval=interval,\n",
    "                             period=period,\n",
    "                             auto_adjust=False,\n",
    "                             prepost=True, # include aftermarket hours\n",
    "                            )\n",
    "    stock_df.columns = stock_df.columns.str.lower().str.replace(' ', '_')\n",
    "    stock_df.to_pickle(f'./data/{symbol}_{interval}_df.pkl')\n",
    "    \n",
    "def load(symbol, interval):\n",
    "    return pd.read_pickle(f'./data/{symbol}_{interval}_df.pkl')\n",
    "\n",
    "def load_model_df(symbol, interval):\n",
    "    return pd.read_pickle(f'./models/{symbol}_{interval}_model_df.pkl')\n",
    "\n",
    "#########################################\n",
    "# functions for use to transform tables #\n",
    "#########################################\n",
    "\n",
    "# candle parts percentages\n",
    "def candle_parts_pcts(o, c, h, l):\n",
    "    full = h - l\n",
    "    if full == 0:\n",
    "        # If full is zero, return 0 for all components to avoid division by zero\n",
    "        return 0, 0, 0\n",
    "    body = abs(o - c)\n",
    "    if o > c:\n",
    "        top_wick = h - o\n",
    "        bottom_wick = c - l\n",
    "    else:\n",
    "        top_wick = h - c\n",
    "        bottom_wick = o - l\n",
    "    return top_wick / full, body / full, bottom_wick / full\n",
    "\n",
    "# previous close and open gap % of pervious candle size\n",
    "def gap_up_down_pct(o, pc, ph, pl):\n",
    "    if o == pc:\n",
    "        return 0\n",
    "    else:\n",
    "        return (o - pc) / (ph - pl)\n",
    "    \n",
    "    \n",
    "# z-score calculation\n",
    "def zscore(x, mu, stdev):\n",
    "    return (x - mu) / stdev\n",
    "\n",
    "# direction calculation:\n",
    "def direction(pctc, mean, stdev):\n",
    "    \n",
    "    pct_pos = mean + 0.43073 / 2 * stdev\n",
    "    pct_neg = mean - 0.43073 / 2 * stdev\n",
    "    if pctc >= pct_pos:\n",
    "        return 1\n",
    "    elif pctc <= pct_neg:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def transform(symbol, interval, period):\n",
    "    \n",
    "    if load(symbol, interval).shape[0] > 0:\n",
    "        df = load(symbol, interval)\n",
    "        \n",
    "    else:\n",
    "        download(symbol, interval, period)\n",
    "        df = load(symbol,interval)\n",
    "    \n",
    "    # Kalman filtering (noise reduction algorithm) \n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "                      observation_matrices = [1],\n",
    "                      initial_state_mean = 0,\n",
    "                      initial_state_covariance = 1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=0.01\n",
    "                     )\n",
    "\n",
    "    state_means, _ = kf.filter(df['adj_close'].values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=df.index)\n",
    "    df['kma'] = state_means\n",
    "    df['sma40'] = df['adj_close'].rolling(window=40).mean().copy()\n",
    "    df['kma_sma40_diff'] = (df['kma'] - df['sma40']).copy()\n",
    "    df['kma_sma40_diff_stdev21'] = df['kma_sma40_diff'].rolling(window=21).std().copy()\n",
    "    df['kma_sma40_diff_mu21'] = df['kma_sma40_diff'].rolling(window=21).mean().copy()\n",
    "\n",
    "    # Calculate Kalman Filter vs SMA40 difference z-score\n",
    "    df['kma_sma40_diff_z21'] = df.apply(lambda row: zscore(row['kma_sma40_diff'], row['kma_sma40_diff_mu21'], row['kma_sma40_diff_stdev21']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #update 1 day table: candle parts %'s\n",
    "    df[['pct_top_wick', 'pct_body', 'pct_bottom_wick']] = df.apply(lambda row: candle_parts_pcts(row['open'], row['close'], row['high'],  row['low']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #stdev of adjusted close\n",
    "    df['top_stdev21'] = df['pct_top_wick'].rolling(window=21).std().copy() \n",
    "    df['body_stdev21'] = df['pct_body'].rolling(window=21).std().copy() \n",
    "    df['bottom_stdev21'] = df['pct_bottom_wick'].rolling(window=21).std().copy()\n",
    "\n",
    "    #mean of adjusted close\n",
    "    df['top_mu21'] = df['pct_top_wick'].rolling(window=21).mean().copy() \n",
    "    df['body_mu21'] = df['pct_body'].rolling(window=21).mean().copy() \n",
    "    df['bottom_mu21'] = df['pct_bottom_wick'].rolling(window=21).mean().copy()\n",
    "\n",
    "    #z-score of adjusted close\n",
    "    df['top_z21'] = df.apply(lambda row: zscore(row['pct_top_wick'], row['top_mu21'], row['top_stdev21']), axis=1, result_type='expand').copy()\n",
    "    df['body_z21'] = df.apply(lambda row: zscore(row['pct_body'], row['body_mu21'], row['body_stdev21']), axis=1, result_type='expand').copy()\n",
    "    df['bottom_z21'] = df.apply(lambda row: zscore(row['pct_bottom_wick'], row['bottom_mu21'], row['bottom_stdev21']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #update 1 day table: % gap btwn current open relative to previous candle size\n",
    "    df['pc'] = df['close'].shift(1).copy()\n",
    "    df['ph'] = df['high'].shift(1).copy()\n",
    "    df['pl'] = df['low'].shift(1).copy()\n",
    "    df['pct_gap_up_down'] = df.apply(lambda row: gap_up_down_pct(row['open'], row['pc'], row['ph'], row['pl']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #stdev of adjusted close\n",
    "    df['ac_stdev5'] = df['adj_close'].rolling(window=5).std().copy() \n",
    "    df['ac_stdev8'] = df['adj_close'].rolling(window=8).std().copy() \n",
    "    df['ac_stdev13'] = df['adj_close'].rolling(window=13).std().copy()\n",
    "\n",
    "    #mean of adjusted close\n",
    "    df['ac_mu5'] = df['adj_close'].rolling(window=5).mean().copy() \n",
    "    df['ac_mu8'] = df['adj_close'].rolling(window=8).mean().copy() \n",
    "    df['ac_mu13'] = df['adj_close'].rolling(window=13).mean().copy()\n",
    "\n",
    "    #z-score of adjusted close\n",
    "    df['ac_z5'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu5'], row['ac_stdev5']), axis=1, result_type='expand').copy()\n",
    "    df['ac_z8'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu8'], row['ac_stdev8']), axis=1, result_type='expand').copy()\n",
    "    df['ac_z13'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu13'], row['ac_stdev13']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #target column: direction: -1, 0, 1\n",
    "    df['adj_close_pctc'] = df['adj_close'].pct_change()\n",
    "    mean = df['adj_close_pctc'].mean()\n",
    "    stdev = df['adj_close_pctc'].std()\n",
    "    df['direction'] = df.apply(lambda row: direction(row['adj_close_pctc'], mean, stdev), axis=1, result_type='expand').copy() \n",
    "\n",
    "    # day of month, week, hour of day\n",
    "    df['day_of_month'] = df.index.day        # Day of the month (1-31)\n",
    "    df['day_of_week'] = df.index.weekday     # Day of the week (0 = Monday, 6 = Sunday)\n",
    "    df['hour_of_day'] = df.index.hour        # Hour of the day (0-23)\n",
    "  \n",
    "    # categorical features\n",
    "    categorical_features = ['day_of_month',\n",
    "                                'day_of_week',\n",
    "                                'hour_of_day']\n",
    "    \n",
    "    # Change data types of categorical columns to 'category'\n",
    "    for column in categorical_features:\n",
    "        df[column] = df[column].astype('category')\n",
    "    \n",
    "    # save 1d file for model building\n",
    "    df[['top_z21', \n",
    "        'body_z21', \n",
    "        'bottom_z21',\n",
    "        'top_z21',\n",
    "        'body_z21',\n",
    "        'bottom_z21',\n",
    "        'pct_gap_up_down',\n",
    "        'ac_z5',\n",
    "        'ac_z8',\n",
    "        'ac_z13',\n",
    "        'kma_sma40_diff_z21',\n",
    "        'adj_close',\n",
    "        'day_of_month',\n",
    "        'day_of_week',\n",
    "        'hour_of_day',\n",
    "        'direction',\n",
    "       ]\n",
    "      ].to_pickle(f'./models/{symbol}_{interval}_model_df.pkl')\n",
    "\n",
    "    \n",
    "def model(symbol, interval):\n",
    "    # Load data\n",
    "    data = load_model_df(symbol, interval)\n",
    "    data.dropna(inplace=True, axis=0)\n",
    "    X = data.drop(columns=['direction'], axis=1)\n",
    "    y = data['direction']\n",
    "    \n",
    "    # Print column names to check for issues\n",
    "    print(\"Columns in X before preprocessing:\")\n",
    "    print(X.columns)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # Check if categorical_features are present in X\n",
    "    categorical_features = ['day_of_month', 'day_of_week', 'hour_of_day']\n",
    "    missing_features = [col for col in categorical_features if col not in X.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Missing categorical features: {missing_features}\")\n",
    "\n",
    "    # Make categorical transformer\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore',sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # This will include all other columns in the transformed output\n",
    "    )\n",
    "    \n",
    "    # Define your models\n",
    "    models = {\n",
    "        'XGBoost': XGBClassifier(random_state=42, n_jobs=-1),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        # 'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=300),\n",
    "        'SVC': SVC(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Create a pipeline that first preprocesses the data and then trains the model\n",
    "    pipelines = {}\n",
    "    for model_name, model in models.items():\n",
    "        pipelines[model_name] = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                ('classifier', model)])\n",
    "    \n",
    "    #     X_validation_dfs = {}\n",
    "    #     y_validation_series = {}\n",
    "    models = {}\n",
    "    \n",
    "    # Create a function to get the column names after transformation\n",
    "    def get_feature_names_out(column_transformer):\n",
    "        feature_names = []\n",
    "        for name, transformer, columns in column_transformer.transformers_:\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out())\n",
    "            else:\n",
    "                feature_names.extend(columns)\n",
    "        return feature_names\n",
    "\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        # Apply the pipeline's preprocessor to the data\n",
    "        X_transformed = pipeline.named_steps['preprocessor'].fit_transform(X)\n",
    "\n",
    "        # Get feature names after transformation\n",
    "        feature_names = get_feature_names_out(pipeline.named_steps['preprocessor'])\n",
    "\n",
    "        # Convert the sparse matrix to a dense array and then to a DataFrame with proper column names\n",
    "        X_transformed = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "        # Store current prediction data \n",
    "        curr_prediction = X_transformed.iloc[-1].copy()\n",
    "\n",
    "        # Drop last row, model can't see this because it is used for prediction\n",
    "        X_transformed = X_transformed.iloc[:-1]\n",
    "        \n",
    "        # Take a 3.5% sample for validation for validation\n",
    "        #         X_validate = X_transformed.sample(frac=0.035)\n",
    "        #         X_transformed.drop(X_validate.index,axis=0,inplace=True)\n",
    "        #         y_validate = y[X_validate.index].copy()\n",
    "        #         y = y[~X_validate.index]\n",
    "        \n",
    "        # Store validate dataframes in validaion_dfs\n",
    "        #         X_validation_dfs[pipeline.named_steps['classifier']] = X_validate\n",
    "        #         y_validation_series[pipeline.named_steps['classifier']] = y_validate\n",
    "\n",
    "        # Now perform train_test_split on the transformed data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_transformed, y[:-1], test_size=0.2, random_state=42)\n",
    "\n",
    "        # cols\n",
    "        cols = X_train.columns\n",
    "        \n",
    "        # Fit and evaluate the model\n",
    "        model = pipeline.named_steps['classifier']\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # store model in models dictionary\n",
    "        models[pipeline.named_steps['classifier']] = model\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        #         accuracy = accuracy_score(y_test, y_pred)\n",
    "        #         precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        #         recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        #         f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        #         print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        #         print(f\"Precision: {precision:.4f}\")\n",
    "        #         print(f\"Recall: {recall:.4f}\")\n",
    "        #         print(f\"F1 Score: {f1:.4f}\")\n",
    "        #         print(\"\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return curr_prediction, models, cols\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85caed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('NVDA', '15m', '1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3cd0aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X before preprocessing:\n",
      "Index(['top_z21', 'body_z21', 'bottom_z21', 'top_z21', 'body_z21',\n",
      "       'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close', 'day_of_month', 'day_of_week',\n",
      "       'hour_of_day'],\n",
      "      dtype='object')\n",
      "Model: XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.84      0.78        80\n",
      "           1       0.77      0.72      0.75        79\n",
      "           2       0.83      0.78      0.80        95\n",
      "\n",
      "    accuracy                           0.78       254\n",
      "   macro avg       0.78      0.78      0.78       254\n",
      "weighted avg       0.78      0.78      0.78       254\n",
      "\n",
      "Model: RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.74        80\n",
      "           1       0.73      0.71      0.72        79\n",
      "           2       0.84      0.75      0.79        95\n",
      "\n",
      "    accuracy                           0.75       254\n",
      "   macro avg       0.75      0.75      0.75       254\n",
      "weighted avg       0.76      0.75      0.75       254\n",
      "\n",
      "Model: SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      1.00      0.48        80\n",
      "           1       1.00      0.04      0.07        79\n",
      "           2       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.33       254\n",
      "   macro avg       0.44      0.35      0.19       254\n",
      "weighted avg       0.41      0.33      0.18       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curr_prediction, models, cols = model('NVDA', '15m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af46966f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_of_month_3          0.000000\n",
       "day_of_month_4          0.000000\n",
       "day_of_month_5          0.000000\n",
       "day_of_month_6          0.000000\n",
       "day_of_month_9          0.000000\n",
       "day_of_month_10         0.000000\n",
       "day_of_month_11         0.000000\n",
       "day_of_month_12         0.000000\n",
       "day_of_month_13         0.000000\n",
       "day_of_month_16         0.000000\n",
       "day_of_month_17         1.000000\n",
       "day_of_month_19         0.000000\n",
       "day_of_month_20         0.000000\n",
       "day_of_month_21         0.000000\n",
       "day_of_month_22         0.000000\n",
       "day_of_month_23         0.000000\n",
       "day_of_month_26         0.000000\n",
       "day_of_month_27         0.000000\n",
       "day_of_month_28         0.000000\n",
       "day_of_month_29         0.000000\n",
       "day_of_month_30         0.000000\n",
       "day_of_week_0           0.000000\n",
       "day_of_week_1           1.000000\n",
       "day_of_week_2           0.000000\n",
       "day_of_week_3           0.000000\n",
       "day_of_week_4           0.000000\n",
       "hour_of_day_4           0.000000\n",
       "hour_of_day_5           0.000000\n",
       "hour_of_day_6           0.000000\n",
       "hour_of_day_7           0.000000\n",
       "hour_of_day_8           0.000000\n",
       "hour_of_day_9           0.000000\n",
       "hour_of_day_10          0.000000\n",
       "hour_of_day_11          0.000000\n",
       "hour_of_day_12          0.000000\n",
       "hour_of_day_13          0.000000\n",
       "hour_of_day_14          0.000000\n",
       "hour_of_day_15          1.000000\n",
       "hour_of_day_16          0.000000\n",
       "hour_of_day_17          0.000000\n",
       "hour_of_day_18          0.000000\n",
       "hour_of_day_19          0.000000\n",
       "top_z21                 0.263548\n",
       "body_z21                0.104822\n",
       "bottom_z21             -0.334547\n",
       "pct_gap_up_down        -0.026086\n",
       "ac_z5                   0.822602\n",
       "ac_z8                   1.030088\n",
       "ac_z13                 -0.299822\n",
       "kma_sma40_diff_z21     -1.455703\n",
       "adj_close             115.579300\n",
       "Name: 1266, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e33359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['day_of_month_3', 'day_of_month_4', 'day_of_month_5', 'day_of_month_6',\n",
       "       'day_of_month_9', 'day_of_month_10', 'day_of_month_11',\n",
       "       'day_of_month_12', 'day_of_month_13', 'day_of_month_16',\n",
       "       'day_of_month_17', 'day_of_month_19', 'day_of_month_20',\n",
       "       'day_of_month_21', 'day_of_month_22', 'day_of_month_23',\n",
       "       'day_of_month_26', 'day_of_month_27', 'day_of_month_28',\n",
       "       'day_of_month_29', 'day_of_month_30', 'day_of_week_0', 'day_of_week_1',\n",
       "       'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'hour_of_day_4',\n",
       "       'hour_of_day_5', 'hour_of_day_6', 'hour_of_day_7', 'hour_of_day_8',\n",
       "       'hour_of_day_9', 'hour_of_day_10', 'hour_of_day_11', 'hour_of_day_12',\n",
       "       'hour_of_day_13', 'hour_of_day_14', 'hour_of_day_15', 'hour_of_day_16',\n",
       "       'hour_of_day_17', 'hour_of_day_18', 'hour_of_day_19', 'top_z21',\n",
       "       'body_z21', 'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
       "       'kma_sma40_diff_z21', 'adj_close'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd59e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650f60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e286e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7554b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e712a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
