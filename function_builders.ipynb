{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be9b57b",
   "metadata": {},
   "source": [
    "## Build Functions for ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da1a875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Requirments (Updated on 9/17/2024)\n",
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648f2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yfinance import Ticker\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "\n",
    "from src import elt as et\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee7df301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(symbol, interval, period):\n",
    "    stock = Ticker(symbol)\n",
    "    stock_df = stock.history(interval=interval,\n",
    "                             period=period,\n",
    "                             auto_adjust=False,\n",
    "                             prepost=True, # include aftermarket hours\n",
    "                            )\n",
    "    stock_df.columns = stock_df.columns.str.lower().str.replace(' ', '_')\n",
    "    stock_df.to_pickle(f'./data/{symbol}_{interval}_df.pkl')\n",
    "    \n",
    "def load(symbol, interval):\n",
    "    return pd.read_pickle(f'./data/{symbol}_{interval}_df.pkl')\n",
    "\n",
    "def load_model_df(symbol, interval):\n",
    "    return pd.read_pickle(f'./models/{symbol}_{interval}_model_df.pkl')\n",
    "\n",
    "#########################################\n",
    "# functions for use to transform tables #\n",
    "#########################################\n",
    "\n",
    "# candle parts percentages\n",
    "def candle_parts_pcts(o, c, h, l):\n",
    "    full = h - l\n",
    "    if full == 0:\n",
    "        # If full is zero, return 0 for all components to avoid division by zero\n",
    "        return 0, 0, 0\n",
    "    body = abs(o - c)\n",
    "    if o > c:\n",
    "        top_wick = h - o\n",
    "        bottom_wick = c - l\n",
    "    else:\n",
    "        top_wick = h - c\n",
    "        bottom_wick = o - l\n",
    "    return top_wick / full, body / full, bottom_wick / full\n",
    "\n",
    "# previous close and open gap % of pervious candle size\n",
    "def gap_up_down_pct(o, pc, ph, pl):\n",
    "    if o == pc:\n",
    "        return 0\n",
    "    else:\n",
    "        return (o - pc) / (ph - pl)\n",
    "    \n",
    "    \n",
    "# z-score calculation\n",
    "def zscore(x, mu, stdev):\n",
    "    return (x - mu) / stdev\n",
    "\n",
    "# direction calculation:\n",
    "def direction(pctc, mean, stdev):\n",
    "    \n",
    "    pct_pos = mean + 0.43073 / 2 * stdev\n",
    "    pct_neg = mean - 0.43073 / 2 * stdev\n",
    "    if pctc >= pct_pos:\n",
    "        return 1\n",
    "    elif pctc <= pct_neg:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def transform(symbol, interval, period):\n",
    "    \n",
    "    if load(symbol, interval).shape[0] > 0:\n",
    "        df = load(symbol, interval)\n",
    "        \n",
    "    else:\n",
    "        download(symbol, interval, period)\n",
    "        df = load(symbol,interval)\n",
    "    \n",
    "    # Kalman filtering (noise reduction algorithm) \n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "                      observation_matrices = [1],\n",
    "                      initial_state_mean = 0,\n",
    "                      initial_state_covariance = 1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=0.01\n",
    "                     )\n",
    "\n",
    "    state_means, _ = kf.filter(df['adj_close'].values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=df.index)\n",
    "    df['kma'] = state_means\n",
    "    df['sma40'] = df['adj_close'].rolling(window=40).mean().copy()\n",
    "    df['kma_sma40_diff'] = (df['kma'] - df['sma40']).copy()\n",
    "    df['kma_sma40_diff_stdev21'] = df['kma_sma40_diff'].rolling(window=21).std().copy()\n",
    "    df['kma_sma40_diff_mu21'] = df['kma_sma40_diff'].rolling(window=21).mean().copy()\n",
    "\n",
    "    # Calculate Kalman Filter vs SMA40 difference z-score\n",
    "    df['kma_sma40_diff_z21'] = df.apply(lambda row: zscore(row['kma_sma40_diff'], row['kma_sma40_diff_mu21'], row['kma_sma40_diff_stdev21']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #update 1 day table: candle parts %'s\n",
    "    df[['pct_top_wick', 'pct_body', 'pct_bottom_wick']] = df.apply(lambda row: candle_parts_pcts(row['open'], row['close'], row['high'],  row['low']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #stdev of adjusted close\n",
    "    df['top_stdev21'] = df['pct_top_wick'].rolling(window=21).std().copy() \n",
    "    df['body_stdev21'] = df['pct_body'].rolling(window=21).std().copy() \n",
    "    df['bottom_stdev21'] = df['pct_bottom_wick'].rolling(window=21).std().copy()\n",
    "\n",
    "    #mean of adjusted close\n",
    "    df['top_mu21'] = df['pct_top_wick'].rolling(window=21).mean().copy() \n",
    "    df['body_mu21'] = df['pct_body'].rolling(window=21).mean().copy() \n",
    "    df['bottom_mu21'] = df['pct_bottom_wick'].rolling(window=21).mean().copy()\n",
    "\n",
    "    #z-score of adjusted close\n",
    "    df['top_z21'] = df.apply(lambda row: zscore(row['pct_top_wick'], row['top_mu21'], row['top_stdev21']), axis=1, result_type='expand').copy()\n",
    "    df['body_z21'] = df.apply(lambda row: zscore(row['pct_body'], row['body_mu21'], row['body_stdev21']), axis=1, result_type='expand').copy()\n",
    "    df['bottom_z21'] = df.apply(lambda row: zscore(row['pct_bottom_wick'], row['bottom_mu21'], row['bottom_stdev21']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #update 1 day table: % gap btwn current open relative to previous candle size\n",
    "    df['pc'] = df['close'].shift(1).copy()\n",
    "    df['ph'] = df['high'].shift(1).copy()\n",
    "    df['pl'] = df['low'].shift(1).copy()\n",
    "    df['pct_gap_up_down'] = df.apply(lambda row: gap_up_down_pct(row['open'], row['pc'], row['ph'], row['pl']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #stdev of adjusted close\n",
    "    df['ac_stdev5'] = df['adj_close'].rolling(window=5).std().copy() \n",
    "    df['ac_stdev8'] = df['adj_close'].rolling(window=8).std().copy() \n",
    "    df['ac_stdev13'] = df['adj_close'].rolling(window=13).std().copy()\n",
    "\n",
    "    #mean of adjusted close\n",
    "    df['ac_mu5'] = df['adj_close'].rolling(window=5).mean().copy() \n",
    "    df['ac_mu8'] = df['adj_close'].rolling(window=8).mean().copy() \n",
    "    df['ac_mu13'] = df['adj_close'].rolling(window=13).mean().copy()\n",
    "\n",
    "    #z-score of adjusted close\n",
    "    df['ac_z5'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu5'], row['ac_stdev5']), axis=1, result_type='expand').copy()\n",
    "    df['ac_z8'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu8'], row['ac_stdev8']), axis=1, result_type='expand').copy()\n",
    "    df['ac_z13'] = df.apply(lambda row: zscore(row['adj_close'], row['ac_mu13'], row['ac_stdev13']), axis=1, result_type='expand').copy()\n",
    "\n",
    "    #target column: direction: -1, 0, 1\n",
    "    df['adj_close_pctc'] = df['adj_close'].pct_change()\n",
    "    mean = df['adj_close_pctc'].mean()\n",
    "    stdev = df['adj_close_pctc'].std()\n",
    "    df['direction'] = df.apply(lambda row: direction(row['adj_close_pctc'], mean, stdev), axis=1, result_type='expand').copy() \n",
    "\n",
    "    # day of month, week, hour of day\n",
    "    df['day_of_month'] = df.index.day        # Day of the month (1-31)\n",
    "    df['day_of_week'] = df.index.weekday     # Day of the week (0 = Monday, 6 = Sunday)\n",
    "    df['hour_of_day'] = df.index.hour        # Hour of the day (0-23)\n",
    "  \n",
    "    # categorical features\n",
    "    categorical_features = ['day_of_month',\n",
    "                                'day_of_week',\n",
    "                                'hour_of_day']\n",
    "    \n",
    "    # Change data types of categorical columns to 'category'\n",
    "    for column in categorical_features:\n",
    "        df[column] = df[column].astype('category')\n",
    "    \n",
    "    # save 1d file for model building\n",
    "    df[['top_z21', \n",
    "        'body_z21', \n",
    "        'bottom_z21',\n",
    "        'top_z21',\n",
    "        'body_z21',\n",
    "        'bottom_z21',\n",
    "        'pct_gap_up_down',\n",
    "        'ac_z5',\n",
    "        'ac_z8',\n",
    "        'ac_z13',\n",
    "        'kma_sma40_diff_z21',\n",
    "        'adj_close',\n",
    "        'day_of_month',\n",
    "        'day_of_week',\n",
    "        'hour_of_day',\n",
    "        'direction',\n",
    "       ]\n",
    "      ].to_pickle(f'./models/{symbol}_{interval}_model_df.pkl')\n",
    "\n",
    "    \n",
    "def model(symbol, interval):\n",
    "    # Load data\n",
    "    data = load_model_df(symbol, interval)\n",
    "    data.dropna(inplace=True, axis=0)\n",
    "    X = data.drop(columns=['direction'], axis=1)\n",
    "    y = data['direction']\n",
    "    \n",
    "    # Print column names to check for issues\n",
    "    print(\"Columns in X before preprocessing:\")\n",
    "    print(X.columns)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # Check if categorical_features are present in X\n",
    "    categorical_features = ['day_of_month', 'day_of_week', 'hour_of_day']\n",
    "    missing_features = [col for col in categorical_features if col not in X.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Missing categorical features: {missing_features}\")\n",
    "\n",
    "    # Make categorical transformer\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # This will include all other columns in the transformed output\n",
    "    )\n",
    "    \n",
    "    # Define your models\n",
    "    models = {\n",
    "        'XGBoost': XGBClassifier(random_state=42, n_jobs=-1),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'LogisticRegression': LogisticRegression(multi_class='multinomial', solver='lbfgs', n_jobs=-1),\n",
    "        'SVC': SVC(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Create a pipeline that first preprocesses the data and then trains the model\n",
    "    pipelines = {}\n",
    "    for model_name, model in models.items():\n",
    "        pipelines[model_name] = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                ('classifier', model)])\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Create a function to get the column names after transformation\n",
    "    def get_feature_names_out(column_transformer):\n",
    "        feature_names = []\n",
    "        for name, transformer, columns in column_transformer.transformers_:\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out())\n",
    "            else:\n",
    "                feature_names.extend(columns)\n",
    "        return feature_names\n",
    "\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        # Apply the pipeline's preprocessor to the data\n",
    "        X_transformed = pipeline.named_steps['preprocessor'].fit_transform(X)\n",
    "\n",
    "        # Get feature names after transformation\n",
    "        feature_names = get_feature_names_out(pipeline.named_steps['preprocessor'])\n",
    "\n",
    "        # Convert the sparse matrix to a dense array and then to a DataFrame with proper column names\n",
    "        X_transformed = pd.DataFrame(X_transformed.toarray(), columns=feature_names)\n",
    "\n",
    "        # Fill predictions dictionary for later \n",
    "        predictions[model_name] = X_transformed.iloc[-1]\n",
    "\n",
    "        # Drop last row, model can't see this because it is used for prediction\n",
    "        X_transformed = X_transformed.iloc[:-1]\n",
    "\n",
    "        # Now perform train_test_split on the transformed data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_transformed, y[:-1], test_size=0.2, random_state=42)\n",
    "\n",
    "        # Fit and evaluate the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce9f1dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1282 entries, 2024-08-19 04:00:00-04:00 to 2024-09-17 04:15:00-04:00\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   open          1282 non-null   float64\n",
      " 1   high          1282 non-null   float64\n",
      " 2   low           1282 non-null   float64\n",
      " 3   close         1282 non-null   float64\n",
      " 4   adj_close     1282 non-null   float64\n",
      " 5   volume        1282 non-null   int64  \n",
      " 6   dividends     1282 non-null   float64\n",
      " 7   stock_splits  1282 non-null   float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 90.1 KB\n"
     ]
    }
   ],
   "source": [
    "download('NVDA', '15m', '1mo')\n",
    "df = load('NVDA', '15m')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b83af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'adj_close', 'volume', 'dividends',\n",
       "       'stock_splits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa6f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('NVDA','15m','1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa99a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_model_df = pd.read_pickle('./models/NVDA_15m_model_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b269f6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "direction\n",
       "0    441\n",
       "1    432\n",
       "2    409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvda_model_df.direction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f133665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1282 entries, 2024-08-19 04:00:00-04:00 to 2024-09-17 04:15:00-04:00\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   top_z21             1262 non-null   float64 \n",
      " 1   body_z21            1262 non-null   float64 \n",
      " 2   bottom_z21          1262 non-null   float64 \n",
      " 3   top_z21             1262 non-null   float64 \n",
      " 4   body_z21            1262 non-null   float64 \n",
      " 5   bottom_z21          1262 non-null   float64 \n",
      " 6   pct_gap_up_down     1281 non-null   float64 \n",
      " 7   ac_z5               1278 non-null   float64 \n",
      " 8   ac_z8               1275 non-null   float64 \n",
      " 9   ac_z13              1270 non-null   float64 \n",
      " 10  kma_sma40_diff_z21  1223 non-null   float64 \n",
      " 11  adj_close           1282 non-null   float64 \n",
      " 12  day_of_month        1282 non-null   category\n",
      " 13  day_of_week         1282 non-null   category\n",
      " 14  hour_of_day         1282 non-null   category\n",
      " 15  direction           1282 non-null   int64   \n",
      "dtypes: category(3), float64(12), int64(1)\n",
      "memory usage: 144.1 KB\n"
     ]
    }
   ],
   "source": [
    "nvda_model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be3cd0aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X before preprocessing:\n",
      "Index(['top_z21', 'body_z21', 'bottom_z21', 'top_z21', 'body_z21',\n",
      "       'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close', 'day_of_month', 'day_of_week',\n",
      "       'hour_of_day'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day_of_month'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:361\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 361\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day_of_month'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNVDA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m15m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 248\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(symbol, interval)\u001b[0m\n\u001b[1;32m    245\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_transformed, y[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Fit and evaluate the model\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Evaluation metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:968\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m    966\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:536\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    534\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    535\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 536\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/Documents/github/stock_playground/myenv/lib/python3.12/site-packages/sklearn/utils/_indexing.py:369\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    366\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "model('NVDA', '15m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_df('NVDA','15m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602731bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X before preprocessing:\n",
      "Index(['top_z21', 'body_z21', 'bottom_z21', 'top_z21', 'body_z21',\n",
      "       'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close', 'day_of_month', 'day_of_week',\n",
      "       'hour_of_day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = load_model_df('NVDA','15m')\n",
    "data.dropna(inplace=True, axis=0)\n",
    "X = data.drop(columns=['direction'], axis=1)\n",
    "y = data['direction']\n",
    "\n",
    "# Print column names to check for issues\n",
    "print(\"Columns in X before preprocessing:\")\n",
    "print(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f129dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove duplicate columns\n",
    "X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "# Check if categorical_features are present in X\n",
    "categorical_features = ['day_of_month', 'day_of_week', 'hour_of_day']\n",
    "missing_features = [col for col in categorical_features if col not in X.columns]\n",
    "if missing_features:\n",
    "    print(f\"Missing categorical features: {missing_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06bd6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # This will include all other columns in the transformed output\n",
    ")\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_jobs=-1),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'LogisticRegression': LogisticRegression(multi_class='multinomial', solver='lbfgs', n_jobs=-1),\n",
    "    'SVC': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Create a pipeline that first preprocesses the data and then trains the model\n",
    "pipelines = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipelines[model_name] = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                            ('classifier', model)])\n",
    "\n",
    "predictions = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88c7af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['day_of_month_3', 'day_of_month_4', 'day_of_month_5', 'day_of_month_6',\n",
      "       'day_of_month_9', 'day_of_month_10', 'day_of_month_11',\n",
      "       'day_of_month_12', 'day_of_month_13', 'day_of_month_16',\n",
      "       'day_of_month_17', 'day_of_month_19', 'day_of_month_20',\n",
      "       'day_of_month_21', 'day_of_month_22', 'day_of_month_23',\n",
      "       'day_of_month_26', 'day_of_month_27', 'day_of_month_28',\n",
      "       'day_of_month_29', 'day_of_month_30', 'day_of_week_0', 'day_of_week_1',\n",
      "       'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'hour_of_day_4',\n",
      "       'hour_of_day_5', 'hour_of_day_6', 'hour_of_day_7', 'hour_of_day_8',\n",
      "       'hour_of_day_9', 'hour_of_day_10', 'hour_of_day_11', 'hour_of_day_12',\n",
      "       'hour_of_day_13', 'hour_of_day_14', 'hour_of_day_15', 'hour_of_day_16',\n",
      "       'hour_of_day_17', 'hour_of_day_18', 'hour_of_day_19', 'top_z21',\n",
      "       'body_z21', 'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close'],\n",
      "      dtype='object')\n",
      "Index(['day_of_month_3', 'day_of_month_4', 'day_of_month_5', 'day_of_month_6',\n",
      "       'day_of_month_9', 'day_of_month_10', 'day_of_month_11',\n",
      "       'day_of_month_12', 'day_of_month_13', 'day_of_month_16',\n",
      "       'day_of_month_17', 'day_of_month_19', 'day_of_month_20',\n",
      "       'day_of_month_21', 'day_of_month_22', 'day_of_month_23',\n",
      "       'day_of_month_26', 'day_of_month_27', 'day_of_month_28',\n",
      "       'day_of_month_29', 'day_of_month_30', 'day_of_week_0', 'day_of_week_1',\n",
      "       'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'hour_of_day_4',\n",
      "       'hour_of_day_5', 'hour_of_day_6', 'hour_of_day_7', 'hour_of_day_8',\n",
      "       'hour_of_day_9', 'hour_of_day_10', 'hour_of_day_11', 'hour_of_day_12',\n",
      "       'hour_of_day_13', 'hour_of_day_14', 'hour_of_day_15', 'hour_of_day_16',\n",
      "       'hour_of_day_17', 'hour_of_day_18', 'hour_of_day_19', 'top_z21',\n",
      "       'body_z21', 'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close'],\n",
      "      dtype='object')\n",
      "Index(['day_of_month_3', 'day_of_month_4', 'day_of_month_5', 'day_of_month_6',\n",
      "       'day_of_month_9', 'day_of_month_10', 'day_of_month_11',\n",
      "       'day_of_month_12', 'day_of_month_13', 'day_of_month_16',\n",
      "       'day_of_month_17', 'day_of_month_19', 'day_of_month_20',\n",
      "       'day_of_month_21', 'day_of_month_22', 'day_of_month_23',\n",
      "       'day_of_month_26', 'day_of_month_27', 'day_of_month_28',\n",
      "       'day_of_month_29', 'day_of_month_30', 'day_of_week_0', 'day_of_week_1',\n",
      "       'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'hour_of_day_4',\n",
      "       'hour_of_day_5', 'hour_of_day_6', 'hour_of_day_7', 'hour_of_day_8',\n",
      "       'hour_of_day_9', 'hour_of_day_10', 'hour_of_day_11', 'hour_of_day_12',\n",
      "       'hour_of_day_13', 'hour_of_day_14', 'hour_of_day_15', 'hour_of_day_16',\n",
      "       'hour_of_day_17', 'hour_of_day_18', 'hour_of_day_19', 'top_z21',\n",
      "       'body_z21', 'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close'],\n",
      "      dtype='object')\n",
      "Index(['day_of_month_3', 'day_of_month_4', 'day_of_month_5', 'day_of_month_6',\n",
      "       'day_of_month_9', 'day_of_month_10', 'day_of_month_11',\n",
      "       'day_of_month_12', 'day_of_month_13', 'day_of_month_16',\n",
      "       'day_of_month_17', 'day_of_month_19', 'day_of_month_20',\n",
      "       'day_of_month_21', 'day_of_month_22', 'day_of_month_23',\n",
      "       'day_of_month_26', 'day_of_month_27', 'day_of_month_28',\n",
      "       'day_of_month_29', 'day_of_month_30', 'day_of_week_0', 'day_of_week_1',\n",
      "       'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'hour_of_day_4',\n",
      "       'hour_of_day_5', 'hour_of_day_6', 'hour_of_day_7', 'hour_of_day_8',\n",
      "       'hour_of_day_9', 'hour_of_day_10', 'hour_of_day_11', 'hour_of_day_12',\n",
      "       'hour_of_day_13', 'hour_of_day_14', 'hour_of_day_15', 'hour_of_day_16',\n",
      "       'hour_of_day_17', 'hour_of_day_18', 'hour_of_day_19', 'top_z21',\n",
      "       'body_z21', 'bottom_z21', 'pct_gap_up_down', 'ac_z5', 'ac_z8', 'ac_z13',\n",
      "       'kma_sma40_diff_z21', 'adj_close'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a function to get the column names after transformation\n",
    "def get_feature_names_out(column_transformer):\n",
    "    feature_names = []\n",
    "    for name, transformer, columns in column_transformer.transformers_:\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            feature_names.extend(transformer.get_feature_names_out())\n",
    "        else:\n",
    "            feature_names.extend(columns)\n",
    "    return feature_names\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    # Apply the pipeline's preprocessor to the data\n",
    "    X_transformed = pipeline.named_steps['preprocessor'].fit_transform(X)\n",
    "\n",
    "    # Get feature names after transformation\n",
    "    feature_names = get_feature_names_out(pipeline.named_steps['preprocessor'])\n",
    "\n",
    "    # Convert the sparse matrix to a dense array and then to a DataFrame with proper column names\n",
    "    X_transformed = pd.DataFrame(X_transformed.toarray(), columns=feature_names)\n",
    "    \n",
    "    # Fill predictions dictionary for later \n",
    "    predictions[model_name] = X_transformed.iloc[-1]\n",
    "\n",
    "    # Drop last row, model can't see this because it is used for prediction\n",
    "    X_transformed = X_transformed.iloc[:-1]\n",
    "\n",
    "    # Now perform train_test_split on the transformed data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y[:-1], test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(X_transformed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Fit and evaluate the model\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Evaluation metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred, average='weighted')\n",
    "#     recall = recall_score(y_test, y_pred, average='weighted')\n",
    "#     f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "#     print(f\"Model: {model.__class__.__name__}\")\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46966f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e33359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd59e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650f60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e286e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7554b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e712a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
